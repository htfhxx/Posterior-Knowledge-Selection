{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import params\n",
    "from utils import init_model, save_models, \\\n",
    "    build_vocab, load_data, get_data_loader, Vocabulary\n",
    "from model import Encoder, KnowledgeEncoder, Decoder, Manager\n",
    "from test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_vocab = params.n_vocab\n",
    "n_layer = params.n_layer\n",
    "n_hidden = params.n_hidden\n",
    "n_embed = params.n_embed\n",
    "n_batch = 4\n",
    "temperature = params.temperature\n",
    "train_path = params.train_path\n",
    "#test_path = params.test_path\n",
    "valid_path = params.valid_path\n",
    "#assert torch.cuda.is_available()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the vocab...\n",
      "loading_data...\n",
      "successfully loaded train data\n",
      "successfully loaded  valid data\n"
     ]
    }
   ],
   "source": [
    "# load vocab\n",
    "print(\"loading the vocab...\")\n",
    "vocab = Vocabulary()\n",
    "with open('data/vocab.json', 'r') as fp:\n",
    "    vocab.stoi = json.load(fp)\n",
    "\n",
    "# load data and change to id\n",
    "print(\"loading_data...\")\n",
    "train_X, train_y, train_K = load_data(train_path, vocab)\n",
    "train_loader = get_data_loader(train_X, train_y, train_K, n_batch)\n",
    "print(\"successfully loaded train data\")\n",
    "valid_X, valid_y, valid_K = load_data(valid_path, vocab)\n",
    "valid_loader = get_data_loader(train_X, train_y, train_K, n_batch)\n",
    "print(\"successfully loaded  valid data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(n_vocab, n_embed, n_hidden, n_layer, vocab).cuda()\n",
    "Kencoder = KnowledgeEncoder(n_vocab, n_embed, n_hidden, n_layer, vocab).cuda()\n",
    "manager = Manager(n_hidden, n_vocab, temperature).cuda()\n",
    "decoder = Decoder(n_vocab, n_embed, n_hidden, n_layer, vocab).cuda()\n",
    "\n",
    "\n",
    "model = [encoder, Kencoder, manager, decoder]\n",
    "parameters = list(encoder.parameters()) + list(Kencoder.parameters()) + \\\n",
    "             list(manager.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.Adam(parameters, lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, Kencoder, manager, decoder = [*model]\n",
    "encoder.train(), Kencoder.train(), manager.train(), decoder.train()\n",
    "parameters = list(encoder.parameters()) + list(Kencoder.parameters()) + \\\n",
    "             list(manager.parameters()) + list(decoder.parameters())\n",
    "NLLLoss = nn.NLLLoss(reduction='mean', ignore_index=params.PAD)\n",
    "KLDLoss = nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_loss = 0\n",
    "k_loss = 0\n",
    "n_loss = 0\n",
    "t_loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/10] Step [0050/30625]: total_loss=13.1708 kldiv_loss=0.4588 bow_loss=6.4951 nll_loss=6.2169\n",
      "Epoch [01/10] Step [0100/30625]: total_loss=12.6347 kldiv_loss=0.5567 bow_loss=6.1887 nll_loss=5.8893\n",
      "Epoch [01/10] Step [0150/30625]: total_loss=12.6278 kldiv_loss=0.4023 bow_loss=6.2926 nll_loss=5.9329\n",
      "Epoch [01/10] Step [0200/30625]: total_loss=12.2591 kldiv_loss=0.1474 bow_loss=6.2633 nll_loss=5.8483\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-18e5bbdd870c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOS\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [n_batch]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mis_teacher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.5\u001b[0m  \u001b[1;31m# teacher forcing ratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\anaconda3\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tx_workspace\\Posterior-Knowledge-Selection\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, k, hidden, encoder_outputs, encoder_mask)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mdecoder\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         '''\n\u001b[0m\u001b[0;32m    236\u001b[0m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [1, n_batch, n_embed]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_mask\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [n_batch, 1, seq_len]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\anaconda3\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tx_workspace\\Posterior-Knowledge-Selection\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden, encoder_outputs, encoder_mask)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [n_batch, seq_len, n_hidden]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mencoder_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [n_batch, seq_len, n_hidden]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_mask\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [n_batch, 1, seq_len]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\tx_workspace\\Posterior-Knowledge-Selection\\model.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, hidden, encoder_outputs, encoder_mask)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;31m# print(encoder_mask)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoder_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mattn_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1e9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m             \u001b[1;31m#attn_scores.masked_fill_(encoder_mask, -1e9)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [n_batch, 1, seq_len]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\anaconda3\\envs\\python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step, (src_X, src_y, src_K, tgt_y) in enumerate(train_loader):\n",
    "    src_X = src_X.cuda()\n",
    "    src_y = src_y.cuda()\n",
    "    src_K = src_K.cuda()\n",
    "    tgt_y = tgt_y.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    \n",
    "    encoder_outputs, hidden, x = encoder(src_X)\n",
    "    #encoder_mask = (src_X == 0)[:, :encoder_outputs.size(0)].unsqueeze(1).byte()\n",
    "    encoder_mask = (src_X == 0)[:, :encoder_outputs.size(0)].unsqueeze(1).bool()\n",
    "    #print(encoder_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    y = Kencoder(src_y)\n",
    "    K = Kencoder(src_K)\n",
    "    prior, posterior, k_i, k_logits = manager(x, y, K)\n",
    "    kldiv_loss = KLDLoss(prior, posterior.detach())\n",
    "\n",
    "    n_vocab = params.n_vocab\n",
    "    seq_len = src_y.size(1) - 1\n",
    "    k_logits = k_logits.repeat(seq_len, 1, 1).transpose(0, 1).contiguous().view(-1, n_vocab)\n",
    "    bow_loss = NLLLoss(k_logits, src_y[:, 1:].contiguous().view(-1))\n",
    "\n",
    "    n_batch = src_X.size(0)\n",
    "    max_len = tgt_y.size(1)\n",
    "\n",
    "    outputs = torch.zeros(max_len, n_batch, n_vocab).cuda()\n",
    "    hidden = hidden[params.n_layer:]\n",
    "    output = torch.LongTensor([params.SOS] * n_batch).cuda()  # [n_batch]\n",
    "    for t in range(max_len):\n",
    "        output, hidden, attn_weights = decoder(output, k_i, hidden, encoder_outputs, encoder_mask)\n",
    "        outputs[t] = output\n",
    "        is_teacher = random.random() < 0.5  # teacher forcing ratio\n",
    "        top1 = output.data.max(1)[1]\n",
    "        output = tgt_y[:, t] if is_teacher else top1\n",
    "\n",
    "    outputs = outputs.transpose(0, 1).contiguous()\n",
    "    nll_loss = NLLLoss(outputs.view(-1, n_vocab),\n",
    "                       tgt_y.contiguous().view(-1))\n",
    "\n",
    "    loss = kldiv_loss + nll_loss + bow_loss\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(parameters, 10.0)\n",
    "    optimizer.step()\n",
    "    b_loss += bow_loss.item()\n",
    "    k_loss += kldiv_loss.item()\n",
    "    n_loss += nll_loss.item()\n",
    "    t_loss += loss.item()\n",
    "    if (step + 1) % 50 == 0:\n",
    "        k_loss /= 50\n",
    "        n_loss /= 50\n",
    "        b_loss /= 50\n",
    "        t_loss /= 50\n",
    "        print(\"Epoch [%.2d/%.2d] Step [%.4d/%.4d]: total_loss=%.4f kldiv_loss=%.4f bow_loss=%.4f nll_loss=%.4f\"\n",
    "              % ( 1, 10,\n",
    "                 step + 1, len(train_loader),\n",
    "                 t_loss, k_loss, b_loss, n_loss))\n",
    "        k_loss = 0\n",
    "        n_loss = 0\n",
    "        b_loss = 0\n",
    "        t_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "from torchnlp.word_to_vector import GloVe\n",
    "from utils import gumbel_softmax\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_layer, vocab=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layer = n_layer\n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.gru = nn.GRU(input_size=n_embed, hidden_size=n_hidden,\n",
    "                          num_layers=n_layer, bidirectional=True)\n",
    "        '''\n",
    "        AttributeError: 'GloVe' object has no attribute 'stoi' \n",
    "        if vocab is None:\n",
    "            self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        else:\n",
    "            embedding = torch.Tensor(n_vocab, n_embed)\n",
    "            vectors = GloVe()\n",
    "            for word in vocab.stoi:\n",
    "                if word in vectors.stoi:\n",
    "                    embedding[vocab.stoi[word]] = vectors[word]\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding)\n",
    "            print(\"encoder embedding is initialized with Glove\")\n",
    "        '''\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        :param X:\n",
    "            Variable of shape (n_batch(B), seq_len(T)), which is utterance\n",
    "        :return:\n",
    "            GRU outputs in shape (T, B, n_hidden(H))\n",
    "            last hidden state in shape (2(bi-directional)*n_layer(L), B, H)\n",
    "            encoded utterance defined at paper in shape (B, 2*H)\n",
    "        '''\n",
    "        n_batch = X.size(0)\n",
    "        inputs = self.embedding(X)\n",
    "        inputs = inputs.transpose(0, 1)\n",
    "        seq_lengths = torch.sum(X > 0, dim=-1)  # [n_batch]\n",
    "        packed_inputs = rnn.pack_padded_sequence(inputs, seq_lengths, enforce_sorted=False)\n",
    "        packed_outputs, hidden = self.gru(packed_inputs)  # hidden: [2*n_layer, n_batch, n_hidden]\n",
    "        last_hidden = hidden.view(self.n_layer, 2, n_batch, self.n_hidden)\n",
    "        f_hidden, b_hidden = last_hidden[-1]\n",
    "        outputs, _ = rnn.pad_packed_sequence(packed_outputs)\n",
    "        outputs = (outputs[:, :, :self.n_hidden] + outputs[:, :, self.n_hidden:])\n",
    "        # outputs: [seq_len, n_batch, n_hidden]\n",
    "        encoded = torch.cat((f_hidden, b_hidden), dim=1)  # encoded: [n_batch, 2*n_hidden]\n",
    "        return outputs, hidden, encoded\n",
    "\n",
    "\n",
    "class KnowledgeEncoder(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_layer, vocab=None):\n",
    "        super(KnowledgeEncoder, self).__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layer = n_layer\n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.gru = nn.GRU(input_size=n_embed, hidden_size=n_hidden,\n",
    "                          num_layers=n_layer, bidirectional=True)\n",
    "        '''\n",
    "        if vocab is None:\n",
    "            self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        else:\n",
    "            embedding = torch.Tensor(n_vocab, n_embed)\n",
    "            vectors = GloVe()\n",
    "            for word in vocab.stoi:\n",
    "                if word in vectors.stoi:\n",
    "                    embedding[vocab.stoi[word]] = vectors[word]\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding)\n",
    "            print(\"Kencoder embedding is initialized with Glove\")\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, K):\n",
    "        '''\n",
    "        :param K:\n",
    "             Variable of shape (B, n_knowledge(N), T) (knowledge)\n",
    "             or (B, T), which is y (response)\n",
    "        :return:\n",
    "            encoded knowledge or encoded response in shape (B, N, 2*H)\n",
    "        '''\n",
    "        if len(K.shape) == 3:  # [n_batch, N, seq_len]\n",
    "            n_batch = K.size(0)\n",
    "            N = K.size(1)\n",
    "            inputs = self.embedding(K)\n",
    "            inputs = inputs.transpose(0, 1)  # [N, n_batch, seq_len, n_embed]\n",
    "            encoded = torch.zeros(N, n_batch, 2*self.n_hidden)\n",
    "            for i in range(N):\n",
    "                k = inputs[i].transpose(0, 1)  # [n_batch, seq_len, n_embed]\n",
    "                seq_lengths = torch.sum(K[:, i] > 0, dim=-1)\n",
    "                packed_inputs = rnn.pack_padded_sequence(k, seq_lengths, enforce_sorted=False)\n",
    "                _, hidden = self.gru(packed_inputs)  # hidden: [2*n_layer, n_batch, n_hidden]\n",
    "                hidden = hidden.view(self.n_layer, 2, n_batch, self.n_hidden)\n",
    "                f_hidden, b_hidden = hidden[-1]\n",
    "                encoded[i] = torch.cat((f_hidden, b_hidden), dim=1)  # encoded: [n_batch, 2*n_hidden]\n",
    "            return encoded.transpose(0, 1).cuda()  # [n_batch, N, 2*n_hidden]\n",
    "\n",
    "        else:  # [n_batch, seq_len]\n",
    "            y = K[:, 1:]\n",
    "            n_batch = y.size(0)\n",
    "            inputs = self.embedding(y)\n",
    "            inputs = inputs.transpose(0, 1)  # [seq_len, n_batch, n_embed]\n",
    "            seq_lengths = torch.sum(y > 0, dim=-1)  # [n_batch]\n",
    "            packed_inputs = rnn.pack_padded_sequence(inputs, seq_lengths, enforce_sorted=False)\n",
    "            _, hidden = self.gru(packed_inputs)  # hidden: [2*n_layer, n_batch, n_hidden]\n",
    "            hidden = hidden.view(self.n_layer, 2, n_batch, self.n_hidden)\n",
    "            f_hidden, b_hidden = hidden[-1]\n",
    "            encoded = torch.cat((f_hidden, b_hidden), dim=1)  # encoded: [n_batch, 2*n_hidden]\n",
    "            return encoded\n",
    "\n",
    "\n",
    "class Manager(nn.Module):\n",
    "    def __init__(self, n_hidden, n_vocab, temperature):\n",
    "        super(Manager, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_vocab = n_vocab\n",
    "        self.temperature = temperature\n",
    "        self.mlp = nn.Sequential(nn.Linear(4*n_hidden, 2*n_hidden))\n",
    "        self.mlp_k = nn.Sequential(nn.Linear(2*n_hidden, n_vocab))\n",
    "\n",
    "    def forward(self, x, y, K):\n",
    "        '''\n",
    "        :param x:\n",
    "            encoded utterance in shape (B, 2*H)\n",
    "        :param y:\n",
    "            encoded response in shape (B, 2*H) (optional)\n",
    "        :param K:\n",
    "            encoded knowledge in shape (B, N, 2*H)\n",
    "        :return:\n",
    "            prior, posterior, selected knowledge, selected knowledge logits for BOW_loss\n",
    "        '''\n",
    "        if y is not None:\n",
    "            prior = F.log_softmax(torch.bmm(x.unsqueeze(1), K.transpose(-1, -2)), dim=-1).squeeze(1)\n",
    "            response = self.mlp(torch.cat((x, y), dim=-1))  # response: [n_batch, 2*n_hidden]\n",
    "            K = K.transpose(-1, -2)  # K: [n_batch, 2*n_hidden, N]\n",
    "            posterior_logits = torch.bmm(response.unsqueeze(1), K).squeeze(1)\n",
    "            posterior = F.softmax(posterior_logits, dim=-1)\n",
    "            k_idx = gumbel_softmax(posterior_logits, self.temperature)  # k_idx: [n_batch, N(one_hot)]\n",
    "            k_i = torch.bmm(K, k_idx.unsqueeze(2)).squeeze(2)  # k_i: [n_batch, 2*n_hidden]\n",
    "            k_logits = F.log_softmax(self.mlp_k(k_i), dim=-1)  # k_logits: [n_batch, n_vocab]\n",
    "            return prior, posterior, k_i, k_logits  # prior: [n_batch, N], posterior: [n_batch, N]\n",
    "        else:\n",
    "            n_batch = K.size(0)\n",
    "            k_i = torch.Tensor(n_batch, 2*self.n_hidden).cuda()\n",
    "            prior = torch.bmm(x.unsqueeze(1), K.transpose(-1, -2)).squeeze(1)\n",
    "            k_idx = prior.max(1)[1].unsqueeze(1)  # k_idx: [n_batch, 1]\n",
    "            for i in range(n_batch):\n",
    "                k_i[i] = K[i, k_idx[i]]\n",
    "            return k_i\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.attn = nn.Linear(2 * n_hidden, n_hidden)\n",
    "        self.v = nn.Parameter(torch.rand(n_hidden))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, encoder_mask=None):  # hidden: [n_batch, n_hidden]\n",
    "        seq_len = encoder_outputs.size(0)  # encoder_outputs: [seq_len, n_batch, n_hidden]\n",
    "        h = hidden.repeat(seq_len, 1, 1).transpose(0, 1)  # [n_batch, seq_len, n_hidden]\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [n_batch, seq_len, n_hidden]\n",
    "        attn_weights = self.score(h, encoder_outputs, encoder_mask)  # [n_batch, 1, seq_len]\n",
    "        return attn_weights\n",
    "\n",
    "    def score(self, hidden, encoder_outputs, encoder_mask=None):\n",
    "        # hidden: [n_batch, seq_len, n_hidden], encoder_outputs: [n_batch, seq_len, n_hidden]\n",
    "        attn_scores = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=-1)))\n",
    "        # attn_scores: [n_batch, seq_len, n_hidden]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [n_batch, 1, n_hidden]\n",
    "        attn_scores = torch.bmm(v, attn_scores.transpose(1, 2))  # [n_batch, 1, seq_len]\n",
    "        print(attn_scores)\n",
    "        print(encoder_mask)\n",
    "        if encoder_mask is not None:\n",
    "            attn_scores.masked_fill_(encoder_mask, -1e9)\n",
    "            #attn_scores.masked_fill_(encoder_mask, -1e9)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # [n_batch, 1, seq_len]\n",
    "        return attn_weights  # [n_batch, 1, seq_len]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):  # Hierarchical Gated Fusion Unit\n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_layer, vocab=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layer = n_layer\n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        '''\n",
    "        if vocab is None:\n",
    "            self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        else:\n",
    "            embedding = torch.Tensor(n_vocab, n_embed)\n",
    "            vectors = GloVe()\n",
    "            for word in vocab.stoi:\n",
    "                if word in vectors.stoi:\n",
    "                    embedding[vocab.stoi[word]] = vectors[word]\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding)\n",
    "            print(\"decoder embedding is initialized with Glove\")\n",
    "        '''\n",
    "        self.attention = Attention(n_hidden)\n",
    "        self.y_weight = nn.Linear(n_hidden, n_hidden)\n",
    "        self.k_weight = nn.Linear(n_hidden, n_hidden)\n",
    "        self.z_weight = nn.Linear(2 * n_hidden, n_hidden)\n",
    "        self.y_gru = nn.GRU(n_embed + n_hidden, n_hidden, n_layer)\n",
    "        self.k_gru = nn.GRU(3 * n_hidden, n_hidden, n_layer)\n",
    "        self.out = nn.Linear(2 * n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, input, k, hidden, encoder_outputs, encoder_mask=None):\n",
    "        '''\n",
    "        :param input:\n",
    "            word_input for current time step, in shape (B)\n",
    "        :param k:\n",
    "            selected knowledge in shape (B, 2*H)\n",
    "        :param hidden:\n",
    "            last hidden state of the decoder, in shape (L, B, H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs in shape (T, B, H)\n",
    "        :param encoder_mask:\n",
    "            encoder mask in shape (B, 1, T)\n",
    "        :return:\n",
    "            decoder output, next hidden state of the decoder, attention weights\n",
    "        '''\n",
    "        embedded = self.embedding(input).unsqueeze(0)  # [1, n_batch, n_embed]\n",
    "        attn_weights = self.attention(hidden[-1], encoder_outputs, encoder_mask)  # [n_batch, 1, seq_len]\n",
    "        context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1))  # [n_batch, 1, n_hidden]\n",
    "        context = context.transpose(0, 1)  # [1, n_batch, n_hidden]\n",
    "        y_input = torch.cat((embedded, context), dim=-1)\n",
    "        k_input = torch.cat((k.unsqueeze(0), context), dim=-1)\n",
    "        y_output, y_hidden = self.y_gru(y_input, hidden)  # y_hidden: [n_layer, n_batch, n_hidden]\n",
    "        k_output, k_hidden = self.k_gru(k_input, hidden)  # k_hidden: [n_layer, n_batch, n_hidden]\n",
    "        t_hidden = torch.tanh(torch.cat((self.y_weight(y_hidden), self.k_weight(k_hidden)), dim=-1))\n",
    "        # t_hidden: [n_layer, n_batch, 2*n_hidden]\n",
    "        r = torch.sigmoid(self.z_weight(t_hidden))  # [n_layer, n_batch, n_hidden]\n",
    "        hidden = torch.mul(r, y_hidden) + torch.mul(1-r, k_hidden) # [n_layer, n_batch, n_hidden]\n",
    "        output = hidden[-1]  # [n_batch, n_hidden]\n",
    "        context = context.squeeze(0)  # [n_batch, 2*n_hidden]\n",
    "        output = self.out(torch.cat((output, context), dim=1))  # [n_batch, n_vocab]\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
